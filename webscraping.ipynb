{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bc3465-2147-4896-aeda-2110d37b49b1",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING\n",
    "## Data mining from Peruvian Open Source Data content provided by the Goverment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5de0b7-c00c-49fe-8b1b-f4870f5ddbe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scope of this Notebook\n",
    "[Datos Abiertos](\"https://www.datosabiertos.gob.pe\") is a raw open data source provide by the goverment of Peru.\n",
    "I used webscrapping methods in **Python** to obtain relevant data from a *energy* sector.\n",
    "At the end of the nombre, a dataset is provided containing the results of the webscrapping and data mining.\n",
    "This is just a taste of what I am able to do for collecting data from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4feedee-a27d-4e89-82c9-a39a0cd79f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this module helps in web scrapping\n",
    "from bs4 import BeautifulSoup\n",
    "# this module helps us to download a web page\n",
    "import requests\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d97263d-ad46-4914-9dbf-dca1cd47ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_labels(df):\n",
    "    \n",
    "    for i, key in enumerate(df.keys()):\n",
    "        if (\"Dataset\" in key) or (\"File\" in key):\n",
    "            continue\n",
    "        else:\n",
    "            df.rename(columns={key: \"Dataset \" + key}, inplace=True)\n",
    "            \n",
    "    df.rename(columns={\"Dataset Fecha de lanzamiento\": \"Dataset Release Date\",\n",
    "                       \"Dataset Fecha modificada \": \"Dataset Modified Date\"},\n",
    "              inplace=True)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101024ba-81c9-488c-a198-e330beb01a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert the http address of the sector you want to analyse:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " https://www.datosabiertos.gob.pe/SEARCH/field_topic/energ%C3%ADa-340?sort_by=changed\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert the http address of the sector you want to analyse:\")\n",
    "http_address = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab568afd-6e8c-45e1-abf5-88c3d5306ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_address = \"https://www.datosabiertos.gob.pe/\"\n",
    "search_field = \"SEARCH/field_topic/\"\n",
    "subfix_field = \"?sort_by=changed\"\n",
    "\n",
    "sector_field = http_address.replace(main_address, \"\").replace(search_field, \"\").replace(subfix_field, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495cffc0-43ee-40e2-9641-3b75ddf8b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = requests.get(http_address).text\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "n_dataset_in_sector = int(re.search(r'\\d+', soup.find(class_=\"view-header\").text).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6712548-d4e4-4789-b28c-407ef5160768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The category ENERG%C3%ADA-340 has 36 data files\n",
      "Only 10 links are shown per page\n",
      "There is a total of 4 pages in ENERG%C3%ADA-340\n"
     ]
    }
   ],
   "source": [
    "#data files in sector\n",
    "n_links_per_page = 10\n",
    "n_pages = int(np.ceil(n_dataset_in_sector/n_links_per_page))\n",
    "\n",
    "print(f\"The category {sector_field.upper()} has {n_dataset_in_sector} data files\")\n",
    "print(f\"Only {n_links_per_page} links are shown per page\")\n",
    "print(f\"There is a total of {n_pages} pages in {sector_field.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bcd5097-6bcd-4e74-8133-189684e5f51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For page 1 with 10 results (35 results for Energy, 4 browsing pages)\n",
    "# getting all URLs\n",
    "URL = []\n",
    "\n",
    "for i in range(n_pages):\n",
    "    query = f\"?query=&sort_by=changed&sort_order=DESC&page=0%2C{i}\"\n",
    "    URL.append(main_address + search_field + sector_field + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990c95d5-de9d-4d9f-8d00-1106e60b4291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pages scanned: 100%|██████████████████████████████| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "children = []\n",
    "\n",
    "for url in tqdm(URL, desc=\"Pages scanned\"):\n",
    "    data  = requests.get(url).text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    tag_view_content = soup.body.find_all(class_=\"view-content\")[0]\n",
    "\n",
    "    for child in tag_view_content.children:\n",
    "        if child != '\\n':\n",
    "            children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521b5fdc-35d8-469e-9a17-2bc0565688ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files to be download are: 36\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of files to be download are: {len(children)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddaa4b4f-2be9-4b1a-8537-73e2f59e31d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Metadata per link: 100%|████████████| 36/36 [00:00<00:00, 820.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_meta_data = pd.DataFrame(columns=[\"Dataset Order\",\n",
    "                                     \"Dataset Title\",\n",
    "                                     \"Dataset Link\",\n",
    "                                     \"Dataset Produced By\",\n",
    "                                     \"Dataset Description\",])\n",
    "\n",
    "for i, child in tqdm(enumerate(children), total=len(children), desc=\"Processing Metadata per link\"):    \n",
    "    item = {}\n",
    "    \n",
    "    item[\"Dataset Order\"] = f\"N_{i:02d}\"\n",
    "    \n",
    "    try:\n",
    "        item[\"Dataset Title\"] = child.find_all(class_=\"node-title\")[0].text\n",
    "    except IndexError or AttributeError:\n",
    "        item[\"Dataset Title\"] = None\n",
    "    \n",
    "    try:\n",
    "        item[\"Dataset Link\"] = main_address + child.h2.a[\"href\"]\n",
    "    except IndexError or AttributeError:\n",
    "        item[\"Dataset Link\"] = None\n",
    "\n",
    "    try:\n",
    "        item[\"Dataset Produced By\"] = child.find_all(class_=\"group-membership\")[0].text\n",
    "    except IndexError or AttributeError:\n",
    "        item[\"Dataset Produced By\"] = None\n",
    "    \n",
    "    try:\n",
    "        item[\"Dataset Description\"] = child.find_all(class_=\"node-description\")[0].p.text\n",
    "    except AttributeError:\n",
    "        item[\"Dataset Description\"] = None\n",
    "    \n",
    "    df_meta_data = pd.concat([df_meta_data, pd.DataFrame([item])], ignore_index=True)\n",
    "    \n",
    "del soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b373d045-e418-4507-81b7-113335824f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning for unique labels: 100%|███████████████| 36/36 [00:39<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting the missing labels for the meta data of each file\n",
    "# They are extracted from the specific page of each dataset inside the energy sector\n",
    "labels = []\n",
    "for link in tqdm(df_meta_data[\"Dataset Link\"], desc=\"Scanning for unique labels\"):\n",
    "    data = requests.get(link).text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")  # create a soup object using the variable 'data'\n",
    "    \n",
    "    for label in soup.table.find_all(class_=\"field-label\"):\n",
    "        labels.append(label.text)\n",
    "        \n",
    "labels = list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f264fe9-a37a-4e8f-a947-a43c3b79c005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_meta_data = pd.concat([df_meta_data, pd.DataFrame(None,\n",
    "                                                     index=range(n_dataset_in_sector),\n",
    "                                                     columns=list(labels))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f39747-300e-4165-8425-3dc904edccca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████████████████████| 36/36 [00:51<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for n, row in tqdm(df_meta_data.iterrows(), total=len(df_meta_data), desc=\"Processing Rows\"):\n",
    "    data = requests.get(row[\"Dataset Link\"]).text\n",
    "    # Create a soup object using the variable 'data'\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    table = soup.find(class_=\"field-group-format group_additional\")\n",
    "    for i, tr in enumerate(table.find_all(\"tr\")):\n",
    "        if i == 0:\n",
    "            continue\n",
    "  \n",
    "        for label in labels:\n",
    "            if label in tr.text:\n",
    "                df_meta_data.at[n, label] = tr.text.split(label)[1]\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28229b9-73c1-4080-b9f5-cdd00578f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files_data = pd.DataFrame(columns=[\"Dataset Link\", \"File Title\", \"File Description\", \"File Format\", \"File Link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919f5306-7f77-452e-b012-1cd2d2283a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting files info per Dataset: 100%|███████████| 36/36 [00:42<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(df_meta_data[\"Dataset Link\"], desc=\"Getting files info per Dataset\"):\n",
    "    data = requests.get(url).text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    tag_view_content = soup.body.find(class_=\"resource-list\")\n",
    "    \n",
    "    item = {}    \n",
    "    \n",
    "    try:\n",
    "        # Since the last row is always empty in \"Dato y Medio de Distribución\" file list, unless there is only one file.\n",
    "        if len(tag_view_content.find_all(\"li\")) == 1:\n",
    "            files_container = tag_view_content.find_all(\"li\")\n",
    "        else:\n",
    "            files_container = tag_view_content.find_all(\"li\")[:-1]\n",
    "        \n",
    "        for tag in files_container:\n",
    "            \n",
    "            item[\"Dataset Link\"] = url\n",
    "        \n",
    "            try:\n",
    "                item[\"File Title\"] = tag.a[\"title\"]\n",
    "            except AttributeError:\n",
    "                item[\"File Title\"] = None\n",
    "\n",
    "            try:\n",
    "                item[\"File Description\"] = tag.p.p.text\n",
    "            except AttributeError:\n",
    "                item[\"File Description\"] = None\n",
    "\n",
    "            try:\n",
    "                item[\"File Format\"] = tag.a.span[\"data-format\"]\n",
    "            except AttributeError:\n",
    "                item[\"File Format\"] = None\n",
    "                \n",
    "            try:\n",
    "                item[\"File Link\"] = tag.find(class_=\"btn btn-primary data-link\")[\"href\"]\n",
    "            except AttributeError:\n",
    "                item[\"File Link\"] = None\n",
    "        \n",
    "            df_files_data = pd.concat([df_files_data, pd.DataFrame([item])], ignore_index=True)\n",
    "            \n",
    "    except:\n",
    "        item = {url, None, None, None, None}\n",
    "        df_files_data = pd.concat([df_files_data, pd.DataFrame([item])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "173c722f-8516-41a1-90e7-d0c9d27d5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.merge(df_meta_data, df_files_data, how=\"inner\", on=[\"Dataset Link\", \"Dataset Link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f11aa6f7-f81c-4c9b-b0c6-9b5f79f3b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_data = replace_labels(df_meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7a6a21f-5874-4a34-856e-25acd4be8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = replace_labels(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55d897d2-ef2c-459e-ad63-208fede17a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_order = ['Dataset Order',\n",
    "               'Dataset Title',\n",
    "               'Dataset Link',\n",
    "               'Dataset Produced By',\n",
    "               'Dataset Description',\n",
    "               'Dataset Identificador',\n",
    "               'Dataset Publisher',\n",
    "               'Dataset Author',\n",
    "               'Dataset Contact Name',\n",
    "               'Dataset Contact Email',\n",
    "               'Dataset Release Date',\n",
    "               'Dataset Modified Date',\n",
    "               'Dataset Temporal Coverage',\n",
    "               'Dataset Frequency',\n",
    "               'Dataset Public Access Level',\n",
    "               'Dataset Homepage URL',\n",
    "               'Dataset Language',\n",
    "               'Dataset License',\n",
    "               'File Title',\n",
    "               'File Description',\n",
    "               'File Format',\n",
    "               'File Link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09f0b47-3d1d-443e-a7e0-98ce9bdb4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result[label_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79252d19-e1d3-490a-b0e2-17499cae27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_data.to_csv(\"df_meta_data.csv\")\n",
    "df_files_data.to_csv(\"df_files_data.csv\")\n",
    "df_result.to_csv(\"df_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
